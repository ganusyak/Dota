{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "from tqdm import tqdm_notebook\n",
    "import os #to access files\n",
    "from scipy.sparse import hstack\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_targets = pd.read_csv(os.path.join(PATH_TO_DATA, \n",
    "                                            'train_targets.csv'), \n",
    "                                   index_col='match_id_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train_targets['radiant_win'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_matches(matches_file):\n",
    "    \n",
    "    MATCHES_COUNT = {\n",
    "        'test_matches.jsonl': 10000,\n",
    "        'train_matches.jsonl': 39675,\n",
    "    }\n",
    "    _, filename = os.path.split(matches_file)\n",
    "    total_matches = MATCHES_COUNT.get(filename)\n",
    "    \n",
    "    with open(matches_file) as fin:\n",
    "        for line in tqdm_notebook(fin, total=total_matches):\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"npc_dota_badguys_siege\":381,\n",
      "    \"npc_dota_creep_badguys_melee\":5062,\n",
      "    \"npc_dota_creep_badguys_ranged\":2595,\n",
      "    \"npc_dota_creep_goodguys_melee\":110,\n",
      "    \"npc_dota_creep_goodguys_ranged\":194,\n",
      "    \"npc_dota_hero_antimage\":1414,\n",
      "    \"npc_dota_hero_centaur\":2496,\n",
      "    \"npc_dota_hero_clinkz\":155,\n",
      "    \"npc_dota_hero_enchantress\":1031,\n",
      "    \"npc_dota_neutral_centaur_khan\":338,\n",
      "    \"npc_dota_neutral_centaur_outrunner\":195,\n",
      "    \"npc_dota_neutral_enraged_wildkin\":253,\n",
      "    \"npc_dota_neutral_satyr_hellcaller\":253,\n",
      "    \"npc_dota_neutral_wildkin\":506\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_list = [] #store data that are read\n",
    "number_of_rows = 50 #how many lines to read \n",
    "\n",
    "#reading data from .jsonl file\n",
    "with open(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')) as fin:\n",
    "    for i in range(number_of_rows):\n",
    "        line = fin.readline()\n",
    "        json_list.append(json.loads(line))\n",
    "        \n",
    "#how many matches to read. For example I took 1\n",
    "for i in range(1, 2):\n",
    "  for j in range(1, 2):#there is 5 players in each team. But I want to look on only one player.\n",
    "    print(json.dumps(json_list[i]['players'][j]['damage'], indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6b3a66c3281e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#now we will look at 4 matches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#and now will take 5 players\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'players'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'damage'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-6b3a66c3281e>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#now we will look at 4 matches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#and now will take 5 players\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'players'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'damage'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5): #now we will look at 4 matches\n",
    "  for j in range(1, 5):#and now will take 5 players\n",
    "    print(json.dumps(list(map(lambda x: x['id'][5:], json_list[i]['players'][j]['damage'])), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def extract_features_csv(match):\n",
    "    \n",
    "    row = [\n",
    "        ('match_id_hash', match['match_id_hash']),\n",
    "    ]\n",
    "\n",
    "    for slot, player in enumerate(match['players']):\n",
    "        if slot < 5:\n",
    "            player_name = 'r%d' % (slot + 1)\n",
    "        else:\n",
    "            player_name = 'd%d' % (slot - 4)\n",
    "\n",
    "        row.append( (f'{player_name}_items', list(map(lambda x: x['id'][5:], player['hero_inventory'])) ) )\n",
    "        #here u can extract other data\n",
    "\n",
    "    return collections.OrderedDict(row)\n",
    "\n",
    "    \n",
    "def extract_targets_csv(match, targets):\n",
    "    return collections.OrderedDict([('match_id_hash', match['match_id_hash'])] + [\n",
    "        (field, targets[field])\n",
    "        for field in ['game_time', 'radiant_win', 'duration', 'time_remaining', 'next_roshan_team']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_from_jsonl(matches_file):\n",
    "  \n",
    "    df_new_features = []\n",
    "\n",
    "    # Process raw data and add new features\n",
    "    for match in read_matches(matches_file):\n",
    "        match_id_hash = match['match_id_hash']\n",
    "        features = extract_features_csv(match)\n",
    "\n",
    "        df_new_features.append(features)\n",
    "\n",
    "    df_new_features = pd.DataFrame.from_records(df_new_features).set_index('match_id_hash')\n",
    "    return df_new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_features_from_jsonl(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = create_features_from_jsonl(os.path.join(PATH_TO_DATA, 'test_matches.jsonl')).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "#Better to save extracted data in files, because extracting takes time...\n",
    "train_df.to_pickle('df_train.pkl')\n",
    "test_df.to_pickle('df_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_items_dummies(train_df, test_df):\n",
    "    \n",
    "    full_df = pd.concat([train_df, test_df], sort=False)\n",
    "    train_size = train_df.shape[0]\n",
    "\n",
    "    for team in 'r', 'd':\n",
    "        players = [f'{team}{i}' for i in range(1, 6)]\n",
    "        item_columns = [f'{player}_items' for player in players]\n",
    "\n",
    "        d = pd.get_dummies(full_df[item_columns[0]].apply(pd.Series).stack()).sum(level=0, axis=0)\n",
    "        dindexes = d.index.values\n",
    "\n",
    "        for c in item_columns[1:]:\n",
    "            d = d.add(pd.get_dummies(full_df[c].apply(pd.Series).stack()).sum(level=0, axis=0), fill_value=0)\n",
    "            d = d.ix[dindexes]\n",
    "\n",
    "        full_df = pd.concat([full_df, d.add_prefix(f'{team}_item_')], axis=1, sort=False)\n",
    "        full_df.drop(columns=item_columns, inplace=True)\n",
    "\n",
    "    train_df = full_df.iloc[:train_size, :]\n",
    "    test_df = full_df.iloc[train_size:, :]\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_consumble_items(train_df, test_df):\n",
    "    \n",
    "    full_df = pd.concat([train_df, test_df], sort=False)\n",
    "    train_size = train_df.shape[0]\n",
    "\n",
    "    for team in 'r', 'd':\n",
    "        consumble_columns = ['tango', 'tpscroll', \n",
    "                             'bottle', 'flask',\n",
    "                            'enchanted_mango', 'clarity',\n",
    "                            'faerie_fire', 'ward_observer',\n",
    "                            'ward_sentry']\n",
    "        \n",
    "        starts_with = f'{team}_item_'\n",
    "        consumble_columns = [starts_with + column for column in consumble_columns]\n",
    "        full_df.drop(columns=consumble_columns, inplace=True)\n",
    "\n",
    "    train_df = full_df.iloc[:train_size, :]\n",
    "    test_df = full_df.iloc[train_size:, :]\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_train = pd.read_pickle('df_train.pkl')\n",
    "new_test = pd.read_pickle('df_test.pkl')\n",
    "\n",
    "new_train, new_test = add_items_dummies(new_train, new_test)\n",
    "new_train, new_test = drop_consumble_items(new_train, new_test)\n",
    "\n",
    "target = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.to_csv('test_items.csv', index=new_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test['match_id_hash']=new_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = new_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = new_test.set_index('match_id_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.to_csv('test_items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train['match_id_hash']=new_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_train.set_index('match_id_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.to_csv('train_items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
